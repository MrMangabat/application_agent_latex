{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from utils import split_text_at_punctuation\n",
    "\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.vectorstores import qdrant\n",
    "\n",
    "from config import GPT_API\n",
    "\n",
    "import re\n",
    "from typing import Annotated, List, Dict, TypedDict\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "from utils import check_latex_safety, validate_words\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = GPT_API\n",
    "\n",
    "current_date = datetime.now().strftime('%B%Y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_TEMPERATURE = 0.2\n",
    "MODEL = \"gpt-4o-2024-05-13\"#\"gpt-3.5-turbo\"\n",
    "\n",
    "LLM_MODEL = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    name=\"Agent for job applications\",\n",
    "    temperature=SET_TEMPERATURE,\n",
    "    n=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = TextLoader(\"ai_engineer_software_engineer.txt\")\n",
    "\n",
    "loader = DirectoryLoader(path=\"jobtemplates/\")\n",
    "\n",
    "load_applications = loader.load()\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    encode_kwargs = {'normalize_embeddings':False}\n",
    ")\n",
    "\n",
    "vectorstore = qdrant.Qdrant.from_documents(\n",
    "    documents=load_applications,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = [\n",
    "    \"Business analytics\",\n",
    "    \"Business maturity\",\n",
    "    \"Strategy\",\n",
    "    \"Non-technical and technical communication\",\n",
    "    \"Algorithms & datastrucures\",\n",
    "    \"Software Engineering\",\n",
    "    \"detail oriented\",\n",
    "    \"Creative thinker\",\n",
    "    \"Problem solving\",\n",
    "    \"Critical thinking\",\n",
    "    \"Team player\",\n",
    "    \"Time management\",\n",
    "    \"Adaptability\",\n",
    "    \"Conflict resolution\",\n",
    "    \"Collaborative\",\n",
    "    \"Dilligent\"\n",
    "]\n",
    "\n",
    "IT_Management = [\n",
    "    \"ITIL\",\n",
    "    \"SAFe\",\n",
    "    \"PRINCE2\",\n",
    "    \"CMMI\",\n",
    "    \"SCRUM\",\n",
    "    \"Agile development\",\n",
    "    \"UML(frequency, class or C4)\",\n",
    "    \"Stakeholder classification\"\n",
    "]\n",
    "\n",
    "Programming_languages = [\n",
    "    \"Python intermediate level\",\n",
    "    \"SQL working understanding\",\n",
    "    \"R working understanding\",\n",
    "    \"JavaScript working understanding\"\n",
    "]\n",
    "\n",
    "technical_skills = [\n",
    "    \"git\"\n",
    "    \"Statistical modelling\",\n",
    "    \"Fundamental Azure knowledge\",\n",
    "    \"PostGres\",\n",
    "    \"Neo4J\",\n",
    "    \"Qdrant\",\n",
    "    \"ANNOY\",\n",
    "    \"Docker\",\n",
    "    \"scraping\",\n",
    "    \"crawling\",\n",
    "    \"MT5\",\n",
    "    \"Bert\",\n",
    "    \"FinBert\",\n",
    "    \"T5\",\n",
    "    \"Scrapy\",\n",
    "    \"Numpy\",\n",
    "    \"Polars\",\n",
    "    \"Pandas\",\n",
    "    \"FastAPI\",\n",
    "    \"VUE3\",\n",
    "    \"TensorFlow2\",\n",
    "    \"Hyggingface\",\n",
    "    \"Pytorch\",\n",
    "    \"SonarCube\",\n",
    "    \"Seaborn(/matplotlib/Plotly)\",\n",
    "    \"PyTest\",\n",
    "    \"SKlearn\"\n",
    "    \"Unsupervised learning: dimensionality reduction, explorative factor analysis, K-mean..\",\n",
    "    \"Supervised learning: Random Forests, multiple logistic regression, SVP, NNs, Classification\"\n",
    "]\n",
    "\n",
    "skills_dict = {\n",
    "    'soft skills': skills,\n",
    "    'IT Management': IT_Management,\n",
    "    'Programming languages': Programming_languages,\n",
    "    'Technical skills': technical_skills\n",
    "}\n",
    "\n",
    "do_not_use_words = [\n",
    "    \"abreast\",\n",
    "    \"ardent\",\n",
    "    \"cruisal\",\n",
    "    \"deeply\",\n",
    "    \"eagerly\",\n",
    "    \"endeavors\",\n",
    "    \"enhance\",\n",
    "    \"enhanced\",\n",
    "    \"enhancing\",\n",
    "    \"extensive\",\n",
    "    \"extensively\", \n",
    "    \"expert\",\n",
    "    \"expertise\",\n",
    "    \"facets\"\n",
    "    \"forefront\",\n",
    "    \"fostering\",\n",
    "    \"fueled\",\n",
    "    \"fulfilling\",\n",
    "    \"honed\",\n",
    "    \"intricacies\",\n",
    "    \"intricate\",\n",
    "    \"meticulous \",\n",
    "    \"perfect\",\n",
    "    \"perfectly\",\n",
    "    \"prowess\",\n",
    "    \"profoundly\",\n",
    "    \"realm\",\n",
    "    \"seamlessly\",\n",
    "    \"specialist\",\n",
    "    \"stems\",\n",
    "    \"thrilled\",\n",
    "    \"versed\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semilarity_document_template [(Document(page_content=\"Data Scientist\\n\\nI reached out due to my interest in AI, Software Development/Engineering, and Knowledge management.\\n\\nMotivation I am driven to progress in the field of AI, particularly focusing on Large Language Models, knowledge extraction, and Natural Language Processing. My goal is to ensure that every AI implementation has a solid foundation of reliable data. I am dedicated to setting up thorough data agreements to back these efforts and am excited about the prospect of working alongside your team. My passion for AI is motivated by genuine curiosity and the satisfaction derived from tackling complex problems. Whether working independently or as part of a team, I am committed to addressing every necessary element to deliver a well-rounded AI solution, including aspects of security and compliance with GDPR and AI-ACT. I am enthusiastic about engaging in an environment that encourages detailed and strategic discussions on AI applications. It aligns perfectly with my career objectives. Contributing to projects that cover DevOps, DevSecOps, and MLOps, and incorporating the latest AI innovations into actionable solutions, is very interesting. The demands of this role, particularly the need for effective communication among various stakeholders combined with a keen interest in continuous learning, position me as a promising long-term candidate with fresh ideas. Addressing all facets essential to a robust AI solution, including security measures and adherence to GDPR & AI-ACT standards, is integral.\\n\\nI am keen to bring my personal interest in AI trends into practical scenarios, engaging with peers who share a similar enthusiasm. I see great potential in directly adding value by developing data pipelines, enhancing knowledge retention, fostering rapid innovation, and considering legal and ethical aspects throughout the AI solution lifecycle through active collaboration. My character thrives in dynamic and challenging settings, making me an ideal fit for the Full-stack Data Scientist role you offer.\\n\\nSkills My experience spans various settings, from everyday to luxury environments in the hospitality industry, where I developed strong communication skills. My prior experiences equipped me to translate business needs into technical requirements effectively. I am committed to sustainable solutions and consistently strive to align project objectives with broader business goals.\\n\\nEducation In the summer of 2023, I completed my Master's degree in Data Science ICT. My thesis utilized foundational transformer models to semi-automatically generate PEST analyses, which were enhanced with Named Entity Recognition and summarization techniques. This project involved analyzing publicly available data to provide macroeconomic insights, particularly concerning market expansion. The objective was to synthesize summaries from vectorized unstructured data, retrieving information based on semantic similarity and ranking through general-purpose language models.\\n\\nAdditionally, I hold a Professional Bachelor's Degree in Economics & IT. For my bachelor's thesis, I developed an enterprise architecture strategy for EnviroProcess Denmark and its parent company, EnviroProcess AB. This strategy was designed to align IT initiatives with the subsidiaryâ€™s core competencies, specific service knowledge, unique industry insights, and overall project execution.\\n\\nContinued Learning I am committed to continuously enhancing my skills and am currently exploring areas such as knowledge engineering, decision intelligence, and multi-agent learning frameworks. Additionally, I open to learn what is needed and I try to stay current with the latest advancements in AI, particularly within the domains of NLP and generative AI.\\n\\nThank you for your time I look forward to the chance to discuss how my background, ambitions, and the alignment with the goals of [Company Name]. I am enthusiastic about potentially contributing to your projects and would appreciate the opportunity to discuss this further.\", metadata={'source': 'jobtemplates/ai_engineer_generative.txt', '_id': '3a77e2f45c7b40cb8b4f218c33361bf9', '_collection_name': 'bf65a23c809b4ee5ad1ec5cd8e4e8b98'}), 0.6413844082895206)]\n"
     ]
    }
   ],
   "source": [
    "query_for_search = \"\"\"\n",
    "\n",
    "\n",
    "Engineering\n",
    "\n",
    "In the platform area in Digital Architecture, Data and AI we lay the groundwork for other product teams to delivering accurate, available, and comprehensive data products to enable actionable insights. In addition to this we create trusted and curated enterprise data products that span the entire business of Vestas. This newly established department is an integral component of Vestas' innovative Digital Powerhouse. Our mission is to empower customers, partners, and colleagues to seamlessly discover, access, and connect essential information for informed decisions and impactful actions.\n",
    "\n",
    " \n",
    "Digital Solutions & Development > Digital Solutions > Chapter - Data Engineering & Architecture\n",
    "\n",
    "  \n",
    "As a Data Engineer in the platform area, you will collaborate closely with colleagues both inside and outside of the platform area.\n",
    " \n",
    "Your role involves leveraging a range of cloud technologies and tools tailored to the specific product you are working on. This encompasses working with technologies and tools such as Snowflake, databricks, dbt (data build tool) and Azure Services (storage accounts, key vaults).\n",
    "\n",
    " \n",
    "\n",
    "Responsibilities\n",
    "\n",
    "Your key responsibilities will be:\n",
    " \n",
    "\n",
    "Designing, constructing, and maintaining scalable, reliable, and efficient data products\n",
    "Managing and monitoring data products\n",
    "Engaging in close collaboration with stakeholders who consume the data product\n",
    "Establishing necessary integrations with various sources to enable data extraction\n",
    " \n",
    " \n",
    "Qualifications \n",
    "\n",
    "Degree in Business Intelligence, Data Engineering or Software Development and/or 3 to 5 years of Professional work experience in a similar field\n",
    "Comfortable in a dynamic and changeable working day\n",
    "Comprehensive analytical and problem-solving skills\n",
    "Advanced communication and collaboration skills, with the ability to work effectively in a cross-functional team environment\n",
    "Proficient communication skills in English (our corporate language), Danish is not required\n",
    " \n",
    "Competencies\n",
    "\n",
    " We envision that you possess experience in designing, building, and maintaining data transformation logic. Furthermore, you may see yourself reflected in any of the following categories:\n",
    "\n",
    "Proficiency in programming languages like SQL and Python - experience with dbt, spark, Airflow or Terraform is considered an asset\n",
    "Experience in the development code in a team using devops techniques and agile development methodologies\n",
    "Expertise in working with various Data Warehouse solutions and constructing data products using technologies such as Snowflake, Databricks, Azure Data Engineering Stack (like storage accounts, key vaults, Synapse, MSSQL, etc.)\n",
    "Understanding of data warehouse modelling methodologies (Kimball, data vault et al.) as well as concepts like data mesh or similar\n",
    " \n",
    "On a personal level, we anticipate that you:\n",
    "\n",
    "Possess a collaborative and open-minded nature, eager to contribute to a globally diverse cross-functional team within the organization\n",
    "Display curiosity and motivation for developing innovative data products that generate value exhibing positive communication skills, coupled with a positive, problem-solving approach to accomplishing tasks\n",
    "Thrive in diverse environments and exhibit flexibility in adapting to evolving conditions embracing a commitment to continuous learning and a desire to contribute to the collective growth of the team\n",
    " \n",
    "What we offer \n",
    "\n",
    "You will join a newly established, innovative, and committed team committed to support the business through the development of enterprise data products. You will also have the possibility to be part of forming how to best build data products. You will experience an environment that promotes continuous learning, enabling you to actualize your ambitions and get the chance to work in an agile office environment. While we hold our team members to high individual standards of collaboration, accountability, and meeting deadlines, we provide unwavering support to one another, collectively celebrating successes and addressing challenges.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "semilarity_document_template = vectorstore.similarity_search_with_score(\n",
    "    query = query_for_search,\n",
    "    k = 1,\n",
    "    score_threshold=0.1)\n",
    "\n",
    "print(\"semilarity_document_template\", semilarity_document_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VACANCY_ANALYSIS_PROMT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"\"\n",
    "                You are an assisatant to a human resource manager//\n",
    "                You are to assist in the analysis of a job vacancy//\n",
    "                Identify vocal points of interest that the company is looking for//\n",
    "                Identify the company name//\n",
    "                Identify the job title//\n",
    "                Identify the skills and technical experience required for the job vacancy provided here to be stored as a dictionary employee skill requirement//\n",
    "                \n",
    "        \"\"\"),\n",
    "\n",
    "        ('human',\"\"\"\n",
    "                Given the job vacancy, you are to analyse the following in detail: {SomeVacantPosition}//\n",
    "                Use these skills {my_skills} to conduct an analysis between job requirements and find matching skills//\n",
    "                Output should contain a list of matching skills required for the job vacancy//\n",
    "                {format_instructions_1}\n",
    "        \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Output data structure\n",
    "class OutputStuctureV1(BaseModel):\n",
    "    company_name: str = Field(description=\"identified company name\")\n",
    "    job_title: str = Field(description=\"identified job title\")\n",
    "    analysis_output: str = Field(description=\"analysis of the job vacancy\")\n",
    "    employees_skills_requirement: dict = Field(description=\"identified skills and technical experience required for the job vacancy\")\n",
    "    matching_skills: dict = Field(description=\"matching skills in the job vacancy\")\n",
    "\n",
    "parser_1 = PydanticOutputParser(pydantic_object=OutputStuctureV1)\n",
    "\n",
    "format_messages = VACANCY_ANALYSIS_PROMT.format(\n",
    "    SomeVacantPosition = query_for_search,\n",
    "    my_skills = skills_dict,\n",
    "    format_instructions_1 = parser_1.get_format_instructions())\n",
    "\n",
    "\n",
    "chain = LLM_MODEL | parser_1 \n",
    "\n",
    "analysis_chain = chain.invoke(format_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('soft_skills', ['Comprehensive analytical and problem-solving skills', 'Advanced communication and collaboration skills', 'Proficient communication skills in English', 'Collaborative and open-minded nature', 'Positive communication skills', 'Problem-solving approach', 'Flexibility in adapting to evolving conditions', 'Commitment to continuous learning']), ('technical_skills', ['Proficiency in programming languages like SQL and Python', 'Experience with dbt, Spark, Airflow, or Terraform', 'Experience in development code using devops techniques and agile methodologies', 'Expertise in working with Data Warehouse solutions like Snowflake, Databricks, Azure Data Engineering Stack', 'Understanding of data warehouse modelling methodologies (Kimball, data vault, etc.)', 'Concepts like data mesh'])]\n",
      "[('soft_skills', ['Problem solving', 'Critical thinking', 'Team player', 'Adaptability', 'Collaborative']), ('technical_skills', ['Python intermediate level', 'SQL working understanding', 'Agile development', 'Fundamental Azure knowledge', 'Numpy', 'Pandas', 'TensorFlow2', 'Pytorch'])]\n"
     ]
    }
   ],
   "source": [
    "# Extracting identified information from analysis_chain\n",
    "identified_company_name = analysis_chain.company_name\n",
    "identified_job_title = analysis_chain.job_title\n",
    "identified_skill_requirements = analysis_chain.employees_skills_requirement\n",
    "identified_matching_skills = analysis_chain.matching_skills\n",
    "identified_analysis_output = analysis_chain.analysis_output\n",
    "\n",
    "# Create the output dictionary according to the schema\n",
    "output = {\n",
    "    \"company_name\": identified_company_name,\n",
    "    \"job_title\": identified_job_title,\n",
    "    \"analysis_output\": identified_analysis_output,\n",
    "    \"employees_skills_requirement\": identified_skill_requirements\n",
    "}\n",
    "\n",
    "# Getting keys from the dictionary\n",
    "get_employee_requirements_keys = identified_skill_requirements.keys()\n",
    "\n",
    "# Create an itemgetter object with these keys\n",
    "get_employee_requirements_lists = itemgetter(*get_employee_requirements_keys)\n",
    "\n",
    "# Applying itemgetter to the dictionary to get the lists\n",
    "employee_requirements = get_employee_requirements_lists(identified_skill_requirements)\n",
    "\n",
    "# Zipping keys with their corresponding lists\n",
    "lists_with_employee_requirements = zip(get_employee_requirements_keys, employee_requirements)\n",
    "\n",
    "# If you need to process lists_with_employee_requirements further, you can do so here\n",
    "get_matching_skills_keys = identified_matching_skills.keys()\n",
    "get_matching_skills_lists = itemgetter(*get_matching_skills_keys)\n",
    "matching_skills = get_matching_skills_lists(identified_matching_skills)\n",
    "lists_with_matching_skills = zip(get_matching_skills_keys, matching_skills)\n",
    "# Example usage:\n",
    "# print the output dictionary\n",
    "print(list(lists_with_employee_requirements))\n",
    "print(list(lists_with_matching_skills))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "         You are to assist in setting up a job application template//\n",
      "         The total amount of characters that can be used is 4000, include white spaces//\n",
      "         \n",
      "         Grammatical correctness is essential//\n",
      "         Use casual business language//\n",
      "         Ensure, the English language is equal to EITLS c1 score//\n",
      "         The template job application must be in English//\n",
      "         200-300 characters for the introduction section//\n",
      "         800-1000 characters for the motivation section//\n",
      "         500-800 characters for the skills section//\n",
      "         560 characters for the masters section//\n",
      "         390 characters for the bachelors section//\n",
      "         300 characters for the continued learning section//\n",
      "         200 characters for the thank you note//\n",
      "         This template is the jobtemplate: [(Document(page_content=\"Data Scientist\\n\\nI reached out due to my interest in AI, Software Development/Engineering, and Knowledge management.\\n\\nMotivation I am driven to progress in the field of AI, particularly focusing on Large Language Models, knowledge extraction, and Natural Language Processing. My goal is to ensure that every AI implementation has a solid foundation of reliable data. I am dedicated to setting up thorough data agreements to back these efforts and am excited about the prospect of working alongside your team. My passion for AI is motivated by genuine curiosity and the satisfaction derived from tackling complex problems. Whether working independently or as part of a team, I am committed to addressing every necessary element to deliver a well-rounded AI solution, including aspects of security and compliance with GDPR and AI-ACT. I am enthusiastic about engaging in an environment that encourages detailed and strategic discussions on AI applications. It aligns perfectly with my career objectives. Contributing to projects that cover DevOps, DevSecOps, and MLOps, and incorporating the latest AI innovations into actionable solutions, is very interesting. The demands of this role, particularly the need for effective communication among various stakeholders combined with a keen interest in continuous learning, position me as a promising long-term candidate with fresh ideas. Addressing all facets essential to a robust AI solution, including security measures and adherence to GDPR & AI-ACT standards, is integral.\\n\\nI am keen to bring my personal interest in AI trends into practical scenarios, engaging with peers who share a similar enthusiasm. I see great potential in directly adding value by developing data pipelines, enhancing knowledge retention, fostering rapid innovation, and considering legal and ethical aspects throughout the AI solution lifecycle through active collaboration. My character thrives in dynamic and challenging settings, making me an ideal fit for the Full-stack Data Scientist role you offer.\\n\\nSkills My experience spans various settings, from everyday to luxury environments in the hospitality industry, where I developed strong communication skills. My prior experiences equipped me to translate business needs into technical requirements effectively. I am committed to sustainable solutions and consistently strive to align project objectives with broader business goals.\\n\\nEducation In the summer of 2023, I completed my Master's degree in Data Science ICT. My thesis utilized foundational transformer models to semi-automatically generate PEST analyses, which were enhanced with Named Entity Recognition and summarization techniques. This project involved analyzing publicly available data to provide macroeconomic insights, particularly concerning market expansion. The objective was to synthesize summaries from vectorized unstructured data, retrieving information based on semantic similarity and ranking through general-purpose language models.\\n\\nAdditionally, I hold a Professional Bachelor's Degree in Economics & IT. For my bachelor's thesis, I developed an enterprise architecture strategy for EnviroProcess Denmark and its parent company, EnviroProcess AB. This strategy was designed to align IT initiatives with the subsidiaryâ€™s core competencies, specific service knowledge, unique industry insights, and overall project execution.\\n\\nContinued Learning I am committed to continuously enhancing my skills and am currently exploring areas such as knowledge engineering, decision intelligence, and multi-agent learning frameworks. Additionally, I open to learn what is needed and I try to stay current with the latest advancements in AI, particularly within the domains of NLP and generative AI.\\n\\nThank you for your time I look forward to the chance to discuss how my background, ambitions, and the alignment with the goals of [Company Name]. I am enthusiastic about potentially contributing to your projects and would appreciate the opportunity to discuss this further.\", metadata={'source': 'jobtemplates/ai_engineer_generative.txt', '_id': '3a77e2f45c7b40cb8b4f218c33361bf9', '_collection_name': 'bf65a23c809b4ee5ad1ec5cd8e4e8b98'}), 0.6413844082895206)]// \n",
      "         \n",
      "        \n",
      "Human: \n",
      "         \n",
      "         I have the following knowledge and skills which can be found in the following dictionary {'soft skills': ['Business analytics', 'Business maturity', 'Strategy', 'Non-technical and technical communication', 'Algorithms & datastrucures', 'Software Engineering', 'detail oriented', 'Creative thinker', 'Problem solving', 'Critical thinking', 'Team player', 'Time management', 'Adaptability', 'Conflict resolution', 'Collaborative', 'Dilligent'], 'IT Management': ['ITIL', 'SAFe', 'PRINCE2', 'CMMI', 'SCRUM', 'Agile development', 'UML(frequency, class or C4)', 'Stakeholder classification'], 'Programming languages': ['Python intermediate level', 'SQL working understanding', 'R working understanding', 'JavaScript working understanding'], 'Technical skills': ['gitStatistical modelling', 'Fundamental Azure knowledge', 'PostGres', 'Neo4J', 'Qdrant', 'ANNOY', 'Docker', 'scraping', 'crawling', 'MT5', 'Bert', 'FinBert', 'T5', 'Scrapy', 'Numpy', 'Polars', 'Pandas', 'FastAPI', 'VUE3', 'TensorFlow2', 'Hyggingface', 'Pytorch', 'SonarCube', 'Seaborn(/matplotlib/Plotly)', 'PyTest', 'SKlearnUnsupervised learning: dimensionality reduction, explorative factor analysis, K-mean..', 'Supervised learning: Random Forests, multiple logistic regression, SVP, NNs, Classification']}//\n",
      "         \n",
      "         write two lines to generate a short introduction with interest in IT and AI with inspiration from the The job vacancy is for a Data Engineer at Vestas. The role involves designing, constructing, and maintaining scalable, reliable, and efficient data products. The candidate will work with cloud technologies such as Snowflake, Databricks, dbt, and Azure Services. Key responsibilities include managing data products, collaborating with stakeholders, and establishing data integrations. The required qualifications include a degree in Business Intelligence, Data Engineering, or Software Development, or 3-5 years of professional experience. The candidate should have strong analytical, problem-solving, and communication skills. Competencies include proficiency in SQL and Python, experience with data warehouse solutions, and familiarity with devops and agile methodologies.//\n",
      "         \n",
      "         write motivation with matching pairs <zip object at 0x7fc47bcf1e00> and <zip object at 0x7fc47bcf24c0> and how these can be utilized for the company's benefi//\n",
      "         \n",
      "         write a section about skills somme of the skills and how they can be utilized for the company's benefit//\n",
      "\n",
      "         keep educational background for later access and save the section about masters degree into latex_edu_master and the section about bachelors into latex_edu_bachelor//\n",
      "         keep continued learning section and provide short context that I am willing to learn what is necessary for the company and specific role//\n",
      "        \n",
      "         write a short and consice thank you note to setup a cofee//\n",
      "\n",
      "         I DO NOT have prior experience in a professional environment in programming, ONLY academia//\n",
      "         I DO have prior experience in project management//\n",
      "         The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"latex_company_name\": {\"title\": \"Latex Company Name\", \"description\": \"Company name\", \"type\": \"string\"}, \"latex_job_title\": {\"title\": \"Latex Job Title\", \"description\": \"Job title\", \"type\": \"string\"}, \"latex_introduction\": {\"title\": \"Latex Introduction\", \"description\": \"Introduction\", \"type\": \"string\"}, \"latex_motivation\": {\"title\": \"Latex Motivation\", \"description\": \"Motivation\", \"type\": \"string\"}, \"latex_skills\": {\"title\": \"Latex Skills\", \"description\": \"Skills\", \"type\": \"string\"}, \"latex_edu_masters\": {\"title\": \"Latex Edu Masters\", \"description\": \"Masters\", \"type\": \"string\"}, \"latex_edu_bachelor\": {\"title\": \"Latex Edu Bachelor\", \"description\": \"Bachelor\", \"type\": \"string\"}, \"latex_continued_learning\": {\"title\": \"Latex Continued Learning\", \"description\": \"Continued learning\", \"type\": \"string\"}, \"latex_thank_you\": {\"title\": \"Latex Thank You\", \"description\": \"Thank you for your time\", \"type\": \"string\"}}, \"required\": [\"latex_company_name\", \"latex_job_title\", \"latex_introduction\", \"latex_motivation\", \"latex_skills\", \"latex_edu_masters\", \"latex_edu_bachelor\", \"latex_continued_learning\", \"latex_thank_you\"]}\n",
      "```\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "TEXT_GENERATION_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',\"\"\"\n",
    "         You are to assist in setting up a job application template//\n",
    "         The total amount of characters that can be used is 4000, include white spaces//\n",
    "         \n",
    "         Grammatical correctness is essential//\n",
    "         Use casual business language//\n",
    "         Ensure, the English language is equal to EITLS c1 score//\n",
    "         The template job application must be in English//\n",
    "         200-300 characters for the introduction section//\n",
    "         800-1000 characters for the motivation section//\n",
    "         500-800 characters for the skills section//\n",
    "         560 characters for the masters section//\n",
    "         390 characters for the bachelors section//\n",
    "         300 characters for the continued learning section//\n",
    "         200 characters for the thank you note//\n",
    "         This template is the jobtemplate: {semilarity_jobtemplate}// \n",
    "         \n",
    "        \"\"\"),\n",
    "\n",
    "        ('human',\"\"\"\n",
    "         \n",
    "         I have the following knowledge and skills which can be found in the following dictionary {skills}//\n",
    "         \n",
    "         write two lines to generate a short introduction with interest in IT and AI with inspiration from the {analysis_output}//\n",
    "         \n",
    "         write motivation with matching pairs {skill_match} and {employee_requirements} and how these can be utilized for the company's benefi//\n",
    "         \n",
    "         write a section about skills somme of the skills and how they can be utilized for the company's benefit//\n",
    "\n",
    "         keep educational background for later access and save the section about masters degree into latex_edu_master and the section about bachelors into latex_edu_bachelor//\n",
    "         keep continued learning section and provide short context that I am willing to learn what is necessary for the company and specific role//\n",
    "        \n",
    "         write a short and consice thank you note to setup a cofee//\n",
    "\n",
    "         I DO NOT have prior experience in a professional environment in programming, ONLY academia//\n",
    "         I DO have prior experience in project management//\n",
    "         {format_instructions_2}\n",
    "        \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Output data structure\n",
    "class OutputStuctureV2(BaseModel):\n",
    "    \n",
    "    latex_company_name: str = Field(description=\"Company name\")\n",
    "    latex_job_title: str = Field(description=\"Job title\")\n",
    "    latex_introduction: str = Field(description=\"Introduction\")\n",
    "    latex_motivation: str = Field(description=\"Motivation\")\n",
    "    latex_skills: str = Field(description=\"Skills\")\n",
    "    latex_edu_masters: str = Field(description=\"Masters\")\n",
    "    latex_edu_bachelor: str = Field(description=\"Bachelor\")\n",
    "    latex_continued_learning: str = Field(description=\"Continued learning\")\n",
    "    latex_thank_you: str = Field(description=\"Thank you for your time\")\n",
    "\n",
    "\n",
    "parser_2 = PydanticOutputParser(pydantic_object=OutputStuctureV2)\n",
    "\n",
    "format_messages_2 = TEXT_GENERATION_PROMPT.format(\n",
    "    analysis_output = identified_analysis_output,\n",
    "    employee_requirements = lists_with_employee_requirements,\n",
    "    skill_match = lists_with_matching_skills,\n",
    "    skills = skills_dict,\n",
    "    semilarity_jobtemplate = semilarity_document_template,\n",
    "    format_instructions_2 = parser_2.get_format_instructions())\n",
    "\n",
    "print(format_messages_2)\n",
    "chain_2 = LLM_MODEL | parser_2\n",
    "\n",
    "analysis_chain_2 = chain_2.invoke(format_messages_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE_WORDS_PROMPT = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         ('system',\"\"\"\n",
    "         \n",
    "#          keep the original text here {analysis_chain_2} and substitube words if they appear on this list {forbidden_words} with something similar //\n",
    "         \n",
    "#         {format_instructions_3}\n",
    "         \n",
    "\n",
    "#         \"\"\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# parser_3 = PydanticOutputParser(pydantic_object=OutputStuctureV2)\n",
    "\n",
    "# format_messages_3 = REMOVE_WORDS_PROMPT.format(\n",
    "#     analysis_chain_2 = analysis_chain_2,\n",
    "#     forbidden_words = do_not_use_words,\n",
    "#     format_instructions_3 = parser_3.get_format_instructions())\n",
    "\n",
    "# print(format_messages_3)\n",
    "\n",
    "# analysis_chain_3 = chain_3.invoke(format_messages_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_company_name='Vestas' latex_job_title='Data Engineer' latex_introduction=\"I am excited to apply for the Data Engineer position at Vestas. My passion for IT and AI, combined with my academic background, aligns perfectly with the role's requirements of designing, constructing, and maintaining scalable, reliable, and efficient data products.\" latex_motivation=\"I am driven by the opportunity to work with cutting-edge cloud technologies such as Snowflake, Databricks, dbt, and Azure Services. My academic experience in data engineering and software development has equipped me with a solid foundation in SQL and Python, which I am eager to apply in a professional setting. I am particularly motivated by the prospect of collaborating with stakeholders to manage and integrate data products, ensuring they meet the highest standards of reliability and efficiency. My strong analytical and problem-solving skills, honed through various academic projects, will enable me to contribute effectively to Vestas' data engineering team. I am also enthusiastic about the company's commitment to agile methodologies and DevOps practices, as they align with my own approach to continuous improvement and innovation. By leveraging my skills and knowledge, I aim to help Vestas achieve its goals of delivering high-quality data solutions that drive business success.\" latex_skills='My skills in business analytics, strategy, and both non-technical and technical communication will be invaluable in translating business needs into technical requirements. My proficiency in Python and SQL, along with my understanding of data warehouse solutions, will enable me to design and maintain efficient data products. Additionally, my experience with agile development and project management methodologies such as SCRUM and PRINCE2 will ensure that projects are delivered on time and meet all stakeholder expectations. My adaptability and problem-solving abilities will help me navigate the dynamic challenges of the data engineering field, making me a valuable asset to Vestas.' latex_edu_masters=\"In the summer of 2023, I completed my Master's degree in Data Science ICT. My thesis utilized foundational transformer models to semi-automatically generate PEST analyses, which were enhanced with Named Entity Recognition and summarization techniques. This project involved analyzing publicly available data to provide macroeconomic insights, particularly concerning market expansion. The objective was to synthesize summaries from vectorized unstructured data, retrieving information based on semantic similarity and ranking through general-purpose language models.\" latex_edu_bachelor=\"Additionally, I hold a Professional Bachelor's Degree in Economics & IT. For my bachelor's thesis, I developed an enterprise architecture strategy for EnviroProcess Denmark and its parent company, EnviroProcess AB. This strategy was designed to align IT initiatives with the subsidiaryâ€™s core competencies, specific service knowledge, unique industry insights, and overall project execution.\" latex_continued_learning='I am committed to continuously enhancing my skills and am currently exploring areas such as knowledge engineering, decision intelligence, and multi-agent learning frameworks. Additionally, I am open to learning what is needed and I try to stay current with the latest advancements in AI, particularly within the domains of NLP and generative AI.' latex_thank_you='Thank you for your time. I look forward to the chance to discuss how my background and ambitions align with the goals of Vestas. I am enthusiastic about potentially contributing to your projects and would appreciate the opportunity to discuss this further over a coffee.'\n"
     ]
    }
   ],
   "source": [
    "print(str(analysis_chain_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 3\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    TypedDict for the graph state.\n",
    "\n",
    "    Args:\n",
    "        TypedDict: Base class for TypedDict.\n",
    "    \"\"\"\n",
    "    error: str\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    generation: str\n",
    "    iterations: int\n",
    "\n",
    "def generate_application(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Generate a job application based on the job template provided.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation.\n",
    "    \"\"\"\n",
    "    print(\"------ Generating application ------\")\n",
    "\n",
    "    # State\n",
    "    messages = state['messages']\n",
    "    iterations = state['iterations']\n",
    "    error = state['error']\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # Generate solution\n",
    "    APPLICATION_OUTPUT_PROMPT = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system',\"\"\"\n",
    "            Validate, that the generated application is not using any of these words found here {forbidden_words} in {general_analysis}//\n",
    "            {format_instructions_3}\n",
    "            \"\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    parser_3 = PydanticOutputParser(pydantic_object=OutputStuctureV2)\n",
    "\n",
    "    formatted_messages = APPLICATION_OUTPUT_PROMPT.format(\n",
    "    general_analysis = analysis_chain_2,\n",
    "    forbidden_words = do_not_use_words,\n",
    "    format_instructions_3 = parser_3.get_format_instructions())\n",
    "\n",
    "    messages += [(role, content) for role, content in formatted_messages]\n",
    "\n",
    "\n",
    "    chain_3 = LLM_MODEL | parser_3\n",
    "\n",
    "    application_solution = chain_3.invoke(messages)\n",
    "    \n",
    "    # Increment iterations\n",
    "    iterations = iterations + 1\n",
    "\n",
    "    return {\n",
    "        \"generation\": application_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": error\n",
    "    }\n",
    "    \n",
    "def check_generation(state: GraphState) -> GraphState:\n",
    "    \"\"\"\n",
    "    Check the generated application for errors.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        state (GraphState): Updated graph state with error status.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"------ Checking application ------\")\n",
    "    # State\n",
    "    messages = state['messages']\n",
    "    iterations = state['iterations']\n",
    "    error = state['error']\n",
    "    application_solution = state['generation']\n",
    "    \n",
    "    # Access solution components\n",
    "    try:\n",
    "        company_name = application_solution.latex_company_name\n",
    "        job_title = application_solution.latex_job_title\n",
    "        introduction = application_solution.latex_introduction\n",
    "        motivation = application_solution.latex_motivation\n",
    "        skills = application_solution.latex_skills\n",
    "        masters = application_solution.latex_edu_masters\n",
    "        bachelors = application_solution.latex_edu_bachelor\n",
    "        continued_learning = application_solution.latex_continued_learning\n",
    "        thank_you = application_solution.latex_thank_you\n",
    "\n",
    "        # Validate words\n",
    "        print('pre IF statement for validate words')\n",
    "        forbidden_words_used = validate_words(do_not_use_words, company_name, job_title, introduction, motivation, skills, continued_learning, thank_you)\n",
    "        if forbidden_words_used:\n",
    "            print('inside IT statement forbidden words used:')\n",
    "            raise ValueError(f\"Forbidden words used: {', '.join(forbidden_words_used)}\")\n",
    "\n",
    "        # # Check LaTeX safety\n",
    "        # safe_texts = check_latex_safety(company_name, job_title, introduction, motivation, skills, continued_learning, thank_you)\n",
    "        # if any(text != original for text, original in zip(safe_texts, [company_name, job_title, introduction, motivation, skills, continued_learning, thank_you])):\n",
    "        #     raise ValueError(\"LaTeX safety issues found in the generated application\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"---APPLICATION CHECK: FAILED---\")\n",
    "        error_message = [(\n",
    "            \"user\", f\"Your generated application failed with: {e} because of {forbidden_words_used}. Reflect and review on this error and prior attempts to solve the issue. #1 state what you think went wrong and #2 try and solve this problem again. Return full solution. Use the output structure with company_name, job_title, introduction, motivation, skills, masters, bachelors, continued_learning, thank_you.\")]\n",
    "        messages += error_message\n",
    "        error = \"yes\"\n",
    "    else:\n",
    "        print(\"---NO APPLICATION ERRORS---\")\n",
    "        error = \"no\"\n",
    "    \n",
    "    return {\n",
    "        \"generation\": application_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": error\n",
    "    }\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    Determines whether to finish or retry based on the error status and iteration count.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call.\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations >= max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\"\n",
    "\n",
    "#### Utilities\n",
    "import uuid \n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: {current_state[-1]}\")\n",
    "\n",
    "    messages = event.get(\"messages\", [])\n",
    "    for role, content in messages:\n",
    "        message_id = f\"{role}_{hash(content)}\"\n",
    "        if message_id not in _printed:\n",
    "            msg_repr = content if len(content) <= max_length else content[:max_length] + \" ... (truncated)\"\n",
    "            print(f\"{role}: {msg_repr}\")\n",
    "            _printed.add(message_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "builder.add_node(\"generate\", generate_application)  # generation solution\n",
    "builder.add_node(\"check_generation\", check_generation)  # check generation\n",
    "\n",
    "# Build graph\n",
    "builder.set_entry_point(\"generate\")\n",
    "builder.add_edge(\"generate\", \"check_generation\")\n",
    "builder.add_conditional_edges(\n",
    "    \"check_generation\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = query_for_search\n",
    "events = graph.stream(\n",
    "    {\"messages\": [('user', question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)\n",
    "    print(\"----\")   \n",
    "event['generation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of allowed words: 459\n",
      "Number of forbidden words used: 2\n",
      "Forbidden words used: ['seamlessly', 'enhancing']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def validate_words(do_not_use_words, *args):\n",
    "    false_count = 0\n",
    "    true_count = 0\n",
    "\n",
    "    forbidden_words_used = []\n",
    "\n",
    "    for arg in args:\n",
    "        words = arg.split()\n",
    "        for word in words:\n",
    "            if word in do_not_use_words:\n",
    "                false_count += 1\n",
    "                forbidden_words_used.append(word)\n",
    "            else:\n",
    "                true_count += 1\n",
    "    \n",
    "    # Return a tuple with the counts and the list of forbidden words used\n",
    "    return true_count, false_count, forbidden_words_used\n",
    "\n",
    "true_count, false_count, forbidden_words_used = validate_words(\n",
    "    do_not_use_words,\n",
    "    final_company_name, final_jobtitle, final_introduction, final_motivation, \n",
    "    final_skills, final_masters, final_bachelors,  final_continued_learning, \n",
    "    final_thank_you\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of allowed words: {true_count}\")\n",
    "print(f\"Number of forbidden words used: {false_count}\")\n",
    "print(f\"Forbidden words used: {forbidden_words_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of safe texts: 4\n",
      "Number of modified texts: 5\n"
     ]
    }
   ],
   "source": [
    "#some regex to remove characters that intervene with latex commands\n",
    "def check_latex_safety(*args):\n",
    "    # Dictionary to map LaTeX special characters to their safe equivalents\n",
    "    replacements = {\n",
    "        '\\\\': ' ',          # backslash to space\n",
    "        '{': ' ',           # curly brace to space\n",
    "        '}': ' ',           # curly brace to space\n",
    "        '#': ' ',           # hash to space\n",
    "        '%': ' ',           # percent to space\n",
    "        '&': 'and',         # ampersand to 'and'\n",
    "        '_': ' ',           # underscore to space\n",
    "        '^': ' ',           # caret to space\n",
    "        '~': ' ',           # tilde to space\n",
    "        '$': 'dollars',     # dollar to space\n",
    "        '/': ' ',           # slash to space\n",
    "        '*': ' ',           # asterisk to space\n",
    "        '-': ' '            # hyphen to space\n",
    "    }\n",
    "    \n",
    "    # Regex pattern to match any LaTeX special character\n",
    "    pattern = r'[\\\\{}#%&_^\\~$\\/\\*\\-]'\n",
    "    \n",
    "    true_count = 0\n",
    "    false_count = 0\n",
    "\n",
    "    # Function to replace matched characters\n",
    "    def replace_match(match):\n",
    "        return replacements[match.group(0)]\n",
    "    \n",
    "    # Process each input text\n",
    "    results = []\n",
    "    for text in args:\n",
    "        if re.search(pattern, text):\n",
    "            false_count += 1\n",
    "            safe_text = re.sub(pattern, replace_match, text)\n",
    "        else:\n",
    "            true_count += 1\n",
    "            safe_text = text\n",
    "        results.append(safe_text)\n",
    "\n",
    "    print(f\"Number of safe texts: {true_count}\")\n",
    "    print(f\"Number of modified texts: {false_count}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "(final_company_name, \n",
    " final_jobtitle, \n",
    " final_introduction, \n",
    " final_motivation, \n",
    " final_skills, \n",
    " final_masters, \n",
    " final_bachelors, \n",
    " final_continued_learning, \n",
    " final_thank_you) = check_latex_safety(\n",
    "                                        final_company_name, \n",
    "                                        final_jobtitle, \n",
    "                                        final_introduction, \n",
    "                                        final_motivation, \n",
    "                                        final_skills, \n",
    "                                        final_masters, \n",
    "                                        final_bachelors, \n",
    "                                        final_continued_learning, \n",
    "                                        final_thank_you)\n",
    "\n",
    "# Directory where the variables.tex file will be saved\n",
    "directory = 'companies_applied_for'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "company_directory = os.path.join(directory, final_company_name)\n",
    "if not os.path.exists(company_directory):\n",
    "    os.makedirs(company_directory)\n",
    "\n",
    "# Write these variables to a .tex file in the specified directory\n",
    "resume_file_path = os.path.join(company_directory, final_company_name + '.tex')\n",
    "with open(resume_file_path, 'w') as text_for_latex:\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalCompanyName}}{{{final_company_name}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalJobtitle}}{{{final_jobtitle}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalIntroduction}}{{{final_introduction}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalMotivation}}{{{final_motivation}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalSkills}}{{{final_skills}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalEducationMaster}}{{{final_masters}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalEducationBachelor}}{{{final_bachelors}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalContinuedLearning}}{{{final_continued_learning}}}\\n\")\n",
    "    text_for_latex.write(f\"\\\\newcommand{{\\\\finalThankYou}}{{{final_thank_you}}}\\n\")\n",
    "\n",
    "latex_filename = f'JMangabat_{final_company_name}_{final_jobtitle}_{current_date}.tex'\n",
    "\n",
    "latex_file_path = os.path.join(company_directory, latex_filename)\n",
    "\n",
    "# Check if the file already exists and create a unique file name if it does\n",
    "file_counter = 1\n",
    "while os.path.exists(latex_file_path):\n",
    "    new_file_name = f'JMangabat_{final_company_name}_{final_jobtitle}_{current_date}_{file_counter}.tex'\n",
    "    latex_file_path = os.path.join(company_directory, new_file_name)\n",
    "    file_counter += 1\n",
    "\n",
    "template_tex = \"main_setup.tex\"\n",
    "\n",
    "tex_content = tex_content = f\"\"\"\n",
    "%----------------------------------------------------------------------------------------\n",
    "% PACKAGES AND OTHER DOCUMENT CONFIGURATIONS\n",
    "%----------------------------------------------------------------------------------------\n",
    "\n",
    "\\\\documentclass{{article}}\n",
    "% \\\\usepackage{{charter}} % Use the Charter font\n",
    "\\\\usepackage{{graphicx}} % Required for including images\n",
    "\\\\usepackage{{fancyhdr}} % Required for customizing headers and footers\n",
    "\\\\usepackage{{setspace}} % Remove paragraph indentation\n",
    "\\\\usepackage{{titlesec}} % Used to customize the \\\\section command\n",
    "\\\\usepackage[\n",
    "    a4paper, % Paper size\n",
    "    top=15mm, % Top margin\n",
    "    bottom=15mm, % Bottom margin\n",
    "    left=15mm, % Left margin\n",
    "    right=15mm, % Right margin\n",
    "    % showframe % Uncomment to show frames around the margins for debugging purposes\n",
    "]{{geometry}}\n",
    "\n",
    "% \\\\setlength{{\\\\parindent}}{{0pt}} % Paragraph indentation\n",
    "\\\\setlength{{\\\\parskip}}{{-0.7em}} % Vertical space between paragraphs\n",
    "\n",
    "\\\\fancypagestyle{{firstpage}}{{%\n",
    "    \\\\fancyhf{{}} % Clear default headers/footers\n",
    "    \\\\renewcommand{{\\\\headrulewidth}}{{0pt}} % No header rule\n",
    "    \\\\renewcommand{{\\\\footrulewidth}}{{1pt}} % Footer rule thickness\n",
    "}}\n",
    "\n",
    "\\\\fancypagestyle{{subsequentpages}}{{%\n",
    "    \\\\fancyhf{{}} % Clear default headers/footers\n",
    "    \\\\renewcommand{{\\\\headrulewidth}}{{1pt}} % Header rule thickness\n",
    "    \\\\renewcommand{{\\\\footrulewidth}}{{1pt}} % Footer rule thickness\n",
    "}}\n",
    "\n",
    "\\\\input{{variables.tex}}\n",
    "\n",
    "\\\\AtBeginDocument{{\\\\thispagestyle{{firstpage}}}} % Use the first page headers/footers style on the first page\n",
    "\\\\pagestyle{{subsequentpages}} % Use the subsequent pages headers/footers style on subsequent pages\n",
    "%----------------------------------------------------------------------------------------\n",
    "%----------------------------------------------------------------------------------------\n",
    "\\\\begin{{document}}\n",
    "\\\\rule{{\\\\linewidth}}{{1pt}} % Horizontal rule\n",
    "\n",
    "% Use the commands in your document\n",
    "\\\\begin{{center}}\n",
    "    Jannik M. B. SÃ¸rensen |\n",
    "    Email: Mangabat93@gmail.com | \n",
    "    Odder, Denmark | Date\n",
    "\\\\end{{center}}\n",
    "\n",
    "\\\\subsection*{{\\\\finalJobtitle, \\\\finalCompanyName}}\n",
    "    \\\\begin{{spacing}}{{1.2}}\n",
    "        {{\\\\finalIntroduction}}\n",
    "    \\\\end{{spacing}}\n",
    "    \\\\vspace*{{0mm}}\n",
    "\n",
    "\\\\subsection*{{Motivation}}\n",
    "    \\\\begin{{spacing}}{{1.2}}\n",
    "        {{\\\\finalMotivation}}\n",
    "    \\\\end{{spacing}}\n",
    "    \\\\vspace*{{0mm}}\n",
    "\n",
    "\\\\subsection*{{Skills}}\n",
    "    \\\\begin{{spacing}}{{1.2}}\n",
    "        {{\\\\finalSkills}}\n",
    "    \\\\end{{spacing}}\n",
    "    \\\\vspace*{{0mm}}\n",
    "\n",
    "\\\\subsection*{{Education}}\n",
    "% Masters degree\n",
    "    \\\\begin{{spacing}}{{1.2}}\n",
    "        {{\\\\finalEducationMaster}}\\\\\\\\\n",
    "        % Bachelors degree\n",
    "        {{\\\\finalEducationBachelor}}\n",
    "    \\\\end{{spacing}}\n",
    "    \\\\vspace*{{0mm}}\n",
    "\n",
    "\\\\subsection*{{Continued Learning}}\n",
    "    \\\\begin{{spacing}}{{1.2}}\n",
    "        {{\\\\finalContinuedLearning}}\n",
    "    \\\\end{{spacing}}\n",
    "    \\\\vspace*{{0mm}}\n",
    "\n",
    "\\\\subsection*{{Thanks for your time}}\n",
    "    \\\\begin{{spacing}}{{1.2}}\n",
    "        {{\\\\finalThankYou}}\\\\\\\\        \n",
    "    \\\\end{{spacing}}\n",
    "\n",
    "\\\\noindent Kind regards,\\\\\\\\\n",
    "    Jannik Mangabat\n",
    "%----------------------------------------------------------------------------------------\n",
    "% LETTER CONTENT\n",
    "%----------------------------------------------------------------------------------------\n",
    "\n",
    "\\\\end{{document}}\n",
    "\"\"\"\n",
    "\n",
    "with open(latex_file_path, 'w') as file:\n",
    "    file.write(tex_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumstuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
